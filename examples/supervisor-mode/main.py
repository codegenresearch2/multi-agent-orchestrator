import os\nimport asyncio\nimport uuid\nfrom typing import Optional, List, Dict, Any\nimport json\nimport sys\n\nfrom multi_agent_orchestrator.orchestrator import MultiAgentOrchestrator, OrchestratorConfig\nfrom multi_agent_orchestrator.agents import (BedrockLLMAgent, BedrockLLMAgentOptions, AgentResponse, AgentCallbacks)\nfrom multi_agent_orchestrator.types import ConversationMessage\nfrom multi_agent_orchestrator.classifiers import BedrockClassifier, BedrockClassifierOptions\n\n# Define a callback class for handling LLM tokens\nclass LLMAgentCallbacks(AgentCallbacks):\n    def on_llm_new_token(self, token: str) -> None:\n        print(token, end='', flush=True)\n\n# Define an async function to handle user requests\nasync def handle_request(_orchestrator: MultiAgentOrchestrator, _user_input: str, _user_id: str, _session_id: str):\n    response: AgentResponse = await _orchestrator.route_request(_user_input, _user_id, _session_id)\n    print('\nMetadata:')\n    print(f'Selected Agent: {response.metadata.agent_name}')\n    if isinstance(response, AgentResponse) and not response.streaming:\n        if isinstance(response.output, str):\n            print(response.output)\n        elif isinstance(response.output, ConversationMessage):\n            print(response.output.content[0].get('text'))\n\n# Define a function to encode custom input payload\ndef custom_input_payload_encoder(input_text: str, chat_history: List[Any], user_id: str, session_id: str, additional_params: Optional[Dict[str, str]] = None) -> str:\n    payload = {\n        'input_text': input_text,\n        'chat_history': chat_history,\n        'user_id': user_id,\n        'session_id': session_id,\n        'additional_params': additional_params if additional_params else {}\n    }\n    return json.dumps(payload)\n\n# Define a function to decode custom output payload\ndef custom_output_payload_decoder(response: Dict[str, Any]) -> Any:\n    decoded_response = json.loads(response['Payload'].read().decode('utf-8'))\n    return ConversationMessage(role='assistant', content=[{'text': decoded_response['response']}])\n\nif __name__ == '__main__':\n    # Initialize the orchestrator with specific configurations\n    orchestrator = MultiAgentOrchestrator(options=OrchestratorConfig(\n        LOG_AGENT_CHAT=True,\n        LOG_CLASSIFIER_CHAT=True,\n        LOG_CLASSIFIER_RAW_OUTPUT=True,\n        LOG_CLASSIFIER_OUTPUT=True,\n        LOG_EXECUTION_TIMES=True,\n        MAX_RETRIES=3,\n        USE_DEFAULT_AGENT_IF_NONE_IDENTIFIED=True,\n        MAX_MESSAGE_PAIRS_PER_AGENT=10,\n    ),\n        classifier=BedrockClassifier(BedrockClassifierOptions(model_id=os.getenv('BEDROCK_MODEL_ID')))\n    )\n\n    # Initialize and add the TechAgent to the orchestrator\n    tech_agent = BedrockLLMAgent(BedrockLLMAgentOptions(\n        name='TechAgent',\n        description='You are a tech agent. You are responsible for answering questions about tech. You are only allowed to answer questions about tech. You are not allowed to answer questions about anything else.',\n        model_id=os.getenv('TECH_AGENT_MODEL_ID'),\n        callbacks=LLMAgentCallbacks()\n    ))\n    orchestrator.add_agent(tech_agent)\n\n    # Define other agents similarly to the gold code\n    # ...\n\n    USER_ID = str(uuid.uuid4())\n    SESSION_ID = str(uuid.uuid4())\n\n    # Print a welcome message and list of available agents\n    print('Welcome to the interactive Multi-Agent system.')\n    print('Here is the list of available agents:')\n    print('- TechAgent: Anything related to technology')\n    # Print other agents similarly\n    # ...\n\n    while True:\n        user_input = input('\nYou: ').strip()\n        if user_input.lower() == 'quit':\n            print('Exiting the program. Goodbye!')\n            sys.exit()\n        asyncio.run(handle_request(orchestrator, user_input, USER_ID, SESSION_ID))