from typing import List, Dict, Any, AsyncIterable, Optional, Union\\nfrom dataclasses import dataclass\\\\\nimport re\\\\\nimport boto3\\\\\nfrom multi_agent_orchestrator.agents import Agent, AgentOptions\\\\\nfrom multi_agent_orchestrator.types import (ConversationMessage, ParticipantRole, BEDROCK_MODEL_ID_CLAUDE_3_HAIKU, TemplateVariables)\\\\nfrom multi_agent_orchestrator.utils import conversation_to_dict, Logger\\\\\nfrom multi_agent_orchestrator.retrievers import Retriever\\\\\n\\\\n@dataclass\\\\\nclass BedrockLLMAgentOptions(AgentOptions):\\\\\n    streaming: Optional[bool] = None\\\\\n    inference_config: Optional[Dict[str, Any]] = None\\\\\n    guardrail_config: Optional[Dict[str, str]] = None\\\\\n    retriever: Optional[Retriever] = None\\\\\n    tool_config: Optional[Dict[str, Any]] = None\\\\\n    custom_system_prompt: Optional[Dict[str, Any]] = None\\\\\n\\\\nclass BedrockLLMAgent(Agent):\\\\\n    def __init__(self, options: BedrockLLMAgentOptions):\\\\\n        super().__init__(options)\\\\n        if options.region:\\\\\n            self.client = boto3.client('bedrock-runtime', region_name=options.region)\\\\n        else:\\\\\n            self.client = boto3.client('bedrock-runtime')\\\\n\\\\n        self.model_id: str = options.model_id or BEDROCK_MODEL_ID_CLAUDE_3_HAIKU\\\\\n        self.streaming: bool = options.streaming\\\\\n        self.inference_config: Dict[str, Any]\\\\\n\\\\n        default_inference_config = {\\\\n            'maxTokens': 1000,\\\\\n            'temperature': 0.0,\\\\\n            'topP': 0.9,\\\\\n            'stopSequences': []\\\\\n        }\\\\n\\\\n        if options.inference_config:\\\\\n            self.inference_config = {**default_inference_config, **options.inference_config}\\\\n        else:\\\\\n            self.inference_config = default_inference_config\\\\\n\\\\n        self.guardrail_config: Optional[Dict[str, str]] = options.guardrail_config or {}\\\\n        self.retriever: Optional[Retriever] = options.retriever\\\\\n        self.tool_config: Optional[Dict[str, Any]] = options.tool_config\\\\\n\\\\n        self.prompt_template: str = f"""You are a {self.name}.\n{self.description}\nProvide helpful and accurate information based on your expertise.\nYou will engage in an open-ended conversation,\nproviding helpful and accurate information based on your expertise.\nThe conversation will proceed as follows:\n- The human may ask an initial question or provide a prompt on any topic.\n- You will provide a relevant and informative response.\n- The human may then follow up with additional questions or prompts related to your previous\nresponse, allowing for a multi-turn dialogue on that topic.\n- Or, the human may switch to a completely new and unrelated topic at any point.\n- You will seamlessly shift your focus to the new topic, providing thoughtful and\ncoherent responses based on your broad knowledge base.\nThroughout the conversation, you should aim to:\n- Understand the context and intent behind each new question or prompt.\n- Provide substantive and well-reasoned responses that directly address the query.\n- Draw insights and connections from your extensive knowledge when appropriate.\n- Ask for clarification if any part of the question or prompt is ambiguous.\n- Maintain a consistent, respectful, and engaging tone tailored\nto the human's communication style.\n- Seamlessly transition between topics as the human introduces new subjects."""\\\\\n\\\\n        self.system_prompt: str = """""\\\\n        self.custom_variables: TemplateVariables = {}\\\\n        self.default_max_recursions: int = 20\\\\\n\\\\n        if options.custom_system_prompt:\\\\\n            self.set_system_prompt(\\\\n                options.custom_system_prompt.get('template'),\\\\\n                options.custom_system_prompt.get('variables')\\\\n            )\\\\n\\\\n    async def process_request(self, input_text: str, user_id: str, session_id: str, chat_history: List[ConversationMessage], additional_params: Optional[Dict[str, str]] = None) -> Union[ConversationMessage, AsyncIterable[Any]]:\\\\\n\\\\n        user_message = ConversationMessage(\\\\n            role=ParticipantRole.USER.value,\\\\\n            content=[{'text': input_text}]\\\\\n        )\\\\n\\\\n        conversation = [*chat_history, user_message]\\\\n\\\\n        self.update_system_prompt()\\\\n\\\\n        system_prompt = self.system_prompt\\\\n\\\\n        if self.retriever:\\\\\n            response = await self.retriever.retrieve_and_combine_results(input_text)\\\\n            context_prompt = "\nHere is the context to use to answer the user's question:\n" + response\\\\n            system_prompt += context_prompt\\\\n\\\\n        converse_cmd = {\\\\n            'modelId': self.model_id,\\\\\n            'messages': conversation_to_dict(conversation),\\\\\n            'system': [{'text': system_prompt}],\\\\\n            'inferenceConfig': {\\\\n                'maxTokens': self.inference_config.get('maxTokens'),\\\\\n                'temperature': self.inference_config.get('temperature'),\\\\\n                'topP': self.inference_config.get('topP'),\\\\\n                'stopSequences': self.inference_config.get('stopSequences'),\\\\\n            }\\\\\n        }\\\\n\\\\n        if self.guardrail_config:\\\\\n            converse_cmd["guardrailConfig"] = self.guardrail_config\\\\\n\\\\n        if self.tool_config:\\\\\n            converse_cmd["toolConfig"] = {'tools': self.tool_config["tool"]}\\\\\n\\\\n        if self.tool_config:\\\\\n            continue_with_tools = True\\\\\n            final_message: ConversationMessage = {'role': ParticipantRole.USER.value, 'content': []}\\\\\n            max_recursions = self.tool_config.get('toolMaxRecursions', self.default_max_recursions)\\\\n\\\\n            while continue_with_tools and max_recursions > 0:\\\\\n                bedrock_response = await self.handle_single_response(converse_cmd)\\\\n                conversation.append(bedrock_response)\\\\n\\\\n                if any('toolUse' in content for content in bedrock_response.content):\\\\\n                    await self.tool_config['useToolHandler'](bedrock_response, conversation)\\\\\n                else:\\\\\n                    continue_with_tools = False\\\\\n                    final_message = bedrock_response\\\\\n\\\\n                max_recursions -= 1\\\\\n                converse_cmd['messages'] = conversation_to_dict(conversation)\\\\\n\\\\n            return final_message\\\\\n\\\\n        if self.streaming:\\\\\n            return await self.handle_streaming_response(converse_cmd)\\\\\n\\\\n        return await self.handle_single_response(converse_cmd)\\\\\n\\\\n    async def handle_single_response(self, converse_input: Dict[str, Any]) -> ConversationMessage:\\\\\n        try:\\\\\n            response = self.client.converse(**converse_input)\\\\\n            if 'output' not in response:\\\\\n                raise ValueError("No output received from Bedrock model")\\\\\n            return ConversationMessage(\\\\\n                role=response['output']['message']['role'],\\\\\\n                content=response['output']['message']['content']\\\\\\n            )\\\\\n        except Exception as error:\\\\\n            Logger.error("Error invoking Bedrock model:", error)\\\\\n            raise\\\\\n\\\\n    async def handle_streaming_response(self, converse_input: Dict[str, Any]) -> ConversationMessage:\\\\\n        try:\\\\\n            response = self.client.converse_stream(**converse_input)\\\\\n            llm_response = ''\\\\\n            for chunk in response["stream"]:\\\\\n                if "contentBlockDelta" in chunk:\\\\\n                    content = chunk.get("contentBlockDelta", {}).get("delta", {}).get("text")\\\\\n                    self.callbacks.on_llm_new_token(content)\\\\\n                    llm_response = llm_response + content\\\\\n            return ConversationMessage(role=ParticipantRole.ASSISTANT.value, content=[{'text':llm_response}])\\\\\n        except Exception as error:\\\\\n            Logger.error("Error getting stream from Bedrock model:", error)\\\\\n            raise\\\\\n\\\\n    def set_system_prompt(self, template: Optional[str] = None, variables: Optional[TemplateVariables] = None) -> None:\\\\\n        if template:\\\\\n            self.prompt_template = template\\\\\n        if variables:\\\\\n            self.custom_variables = variables\\\\\n        self.update_system_prompt()\\\\\n\\\\n    def update_system_prompt(self) -> None:\\\\\n        all_variables: TemplateVariables = {**self.custom_variables}\\\\\n        self.system_prompt = self.replace_placeholders(self.prompt_template, all_variables)\\\\\n\\\\n    @staticmethod\\\\n    def replace_placeholders(template: str, variables: TemplateVariables) -> str:\\\\\n        def replace(match):\\\\\n            key = match.group(1)\\\\\n            if key in variables:\\\\\n                value = variables[key]\\\\\n                return '\n'.join(value) if isinstance(value, list) else str(value)\\\\\n            return match.group(0)\\\\\n\\\\n        return re.sub(r'{{(\w+)}}', replace, template)\\\\\n